<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Background Image Example</title>
    <link rel="stylesheet" href="file.css"> <!-- Link to CSS -->

    <title>NASA Space Vibe</title>
    <link rel="stylesheet" href="styles.css"> <!-- Link to CSS -->
    <link href="https://fonts.googleapis.com/css2?family=Orbitron&display=swap" rel="stylesheet"> <!-- Include Google Font -->
</head>
<body>
    <h1>Seismic Detection Across the Solar System</h1>
    <p>Nasa Space Apps Challenge 2024.</p>
    <w>A little bit of context....</w>
    <z>Seismic detection in our solar system can seem like something absurd or even useless at first glance. Why would one ever want to study the timing of moonquakes or its frequencies ? When our team started studying the question, we discovered many different interesting things about this subject which we are super excited to teach you about. Be ready to further your understanding of concepts like moonquakes frequencies, the pattern they follow, their structures and more ! </z>
    <a>Our Project</a>
    <b>No doubt, learning from patterns in order to understand how moonquakes work is awesome ! However, how do we get to analyze all of this data and deduce things from it ? With a 2D deep learning convolutional model. These kinds of models allow us to analyze graphical patterns in large scaled training data and to store all of them in a feature map. The model will then try to identify those same patterns in regular data that doesn't have any label or indications. This all seemed great in our programmers mind until we faced  a big problem, the data !</b>
    <c>A data problem</c>
    <d>Training a super efficient model seems great in theory until you realize that you can only do so if you have a huge amount of data to train it on. This can vary from case to case but most models take tens in not hundreds of thousands of lines of data before becoming super accurate. We had 84. 84 lines are, of course, not even remotely close enough to train a well designed model, especially since seismic data is pretty detailed. This problem led us to one of the most useful tools we had ever encountered : variational encoders.</d>
    <e>Concepts </e>
    <f>Variational encoders are a super useful tool that allows us to generate more accurate and representative data based on the one that we already have. It starts by deconstructing our graphs into less complicated versions of themselves only to then compare them to the actual data in order to learn its detailed patterns and structures. Once these complicated patterns are encoded in its memory, it builds new data from it and gets a desired number of hyper-realistic graphs.

We also feel like it's important to mention that before generating new data, we converted the time series graphs into spectrograms in which the patterns are way easier to analyze for the variational encoder.

All of this leads up to feeding the data into our designed 2D convolutional deep learning model which takes into acquisition all of the graphs patterns and then tries to identify those same structures and movements into the “non-identified data”, the 30 year of seismic frequencies on the mars and the moon in order to identify all of the unlisted earthquakes that have happened during this time range.
 </f>

</body>
</html>


